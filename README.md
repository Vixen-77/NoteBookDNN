# üî¨ Human Vital Signs Analysis with DNN (Deep Neural Network)

Ce projet vise √† analyser les signes vitaux humains pour pr√©dire le **risque m√©dical (faible ou √©lev√©)** √† l'aide d'un **r√©seau de neurones artificiel** entra√Æn√© sur un jeu de donn√©es biom√©triques.

## üìÅ Contenu du projet

Ce d√©p√¥t contient :
- Le notebook Jupyter d√©velopp√© sous **Google Colab** puis migr√© dans **VS Code**
- Les fichiers CSV pour l'entra√Ænement, validation et test
- Les versions normalis√©es des datasets
- Un scaler sauvegard√© (`.pkl`) pour r√©utilisation

## üß† Objectif

Cr√©er un mod√®le de Deep Learning (DNN) pour pr√©dire la variable `Risk_Category` (Low Risk ou High Risk) √† partir de donn√©es biom√©triques telles que :
- Gender
- Heart rate
- Respiratory rate
- Temperature
- Oxygen saturation
- Blood pressure
- etc.

## üõ†Ô∏è √âtapes r√©alis√©es

1. **Chargement et exploration** du dataset `human_vital_signs_dataset_2024.csv`
2. **Nettoyage** : suppression des colonnes inutiles (`Patient ID`, `Timestamp`)
3. **Encodage** : transformation de `Gender` en valeurs num√©riques (Male ‚Üí 1, Female ‚Üí 0)
4. **M√©lange du dataset** pour √©viter tout biais d‚Äôordre
5. **S√©paration** en 3 fichiers : `train.csv`, `val.csv`, `test.csv`
6. **Normalisation** des donn√©es avec `StandardScaler` (sklearn)
   - Enregistrement du scaler dans un fichier `.pkl`
   - Sauvegarde des fichiers normalis√©s : `train_normalized.csv`, `val_normalized.csv`, `test_normalized.csv`
7. **Construction du mod√®le DNN** avec TensorFlow/Keras :
    **nombre de couche cach√©es** : 5
    **nombre de noeurones par couche** :
       entr√© : 14 param√®tre normalis√© 
       couche1
       couche2
       couche3
       couche4
       couche5
       couche 6 (sortie soit 0 soit 1)
   - les couches 1-5 Dense avec `ReLU`
   - Couches `Dropout` pour √©viter le surapprentissage
   - Derni√®re couche en `sigmoid` pour une sortie binaire

     

8. **Compilation** du mod√®le :
   - Optimiseur : Adam
   - Fonction de perte : Binary Crossentropy
   - M√©trique : Accuracy

## üß™ Entra√Ænement et taille choisi
  
- Nombre d‚Äô√©pochs : `20`
- Batch size : `32`
- Donn√©es d'entra√Ænement : `train_normalized.csv`
- taille de donn√©es d'entrainement : 80% ( exactement 160016 echontillons)

- Donn√©es de Validation : `test_normalized.csv`
-taille de donn√©es de test : 10% (exactement  20002)

- Donn√©es de Validation : `val_normalized.csv`
-taille de donn√©es de validation : 10%   --> les valeurs ici seront utilis√© pour les test de API au moment du sign up on prend les valeur du poid , taille , age et gender dici (exactement 20002)

  ## EN somme : 160016 +20002 +20002 = 200020 